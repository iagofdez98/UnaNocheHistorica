# -*- coding: utf-8 -*-
"""SprintFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ieTtbytheiPXHBpHDqL8JSQ2e8pht_eo

# **SPRINT 3**

# **SISTEMA DE VALORACIÓN DE ITEMS**

El objetivo de este sprint es mostrar el proceso desde la elección de entre las posibles preguntas por el usuario para aproximar un personaje y su posterior procesado hasta mostrar los resultados más parecidos en el dataframe según sus respuestas al test.

# SCRIPT PARA CUBRIR LAS PREGUNTAS

Primero se ejecuta el script que permitirá al usuario responder a las preguntas del test, que disponen de palabras clave relacionadas con la descripción de cada personje para facilitar el procesado. Las respuestas se almacenaran en un txt.
"""

a
textofinal=""
Pregunta1=[
"1. Cual de los siguientes temas sería el elegido para una conversación?" ,
"A ->Politica", "Loved discussing political and philosophical ideas in symposiums, which attracted many scholars. "
,"B ->Famosos" ,"Joined societys effort to encourage art, from music to theatrics including Paint and sculpture. "
,"C ->Sexo", "Known for his affairs, he was involved in various scandals and orgies "
,"D ->¿Hablar, yo prefiero beber", "Fought in several wars getting numerous condecorations. "]

Pregunta2=[
"2. Te ofrecen una pastilla de éxtasis en medio de la noche, que haces?"
,"A ->Me la tomo", "He was described as reckless, and acted impulsively "
,"B ->Hago como que me la tomo, pero la tiro para parecer interesante" ,"He was heavily influenced by the ruler of his time. "
,"C ->Digo que no, y no la cojo", "He denied pleasure, often close to estoic practices Buddhism and fanatic religion. "
,"D ->La cojo, pero se la ofrezco a mis amigos", "He defended the ideas of democracy, and values such as generosity and social welfare. "
]

Pregunta3=[
"3. Te piden que organices la música de una fiesta, sin decirte quien irá ni que se celebra, con la condición de que la música sea de un solo género. ¿Cuál elijes?"
,"A ->Reggaeton", "Who lived in the 20th and 21st century. "
,"B ->Pop" ,"Who lived in the modern age. "
,"C ->Rock duro", ": In the 10th and 11th century. "
,"D ->Música clásica", "In the 1st, 2nd and 3rd century. "
]


Pregunta4=[
"4. ¿Cual de las siguientes noticias te haría más feliz?"
,"A ->Han descubierto la cura del cáncer", "Researcher,  scientist and medicine. "
,"B ->Se ha establecido vida en marte" ,"Interested in exploration and trade. "
,"C ->Se ha acabado con el hambre en el mundo", "Engaged in activism, diplomacy and non violent protest and revolution. "
,"D ->Todos los gobiernos son ideales y hay justicia en el mundo", "Interested in law, politics and ruling. "
]


Pregunta5=[
"5. ¿Qué tipo de alcohol bebes de fiesta?"
,"A ->Cerveza", "I like to party with people of Czech Republic, Austria, Namibia, Germany, Poland, Ireland, Romania, Estonia, Lithuania and Spain. "
,"B ->Vodka" ,"I like to party with people of Russia, United States, and Ukraine. "
,"C ->Tequila" , "I like to party with people of United States, Mexico, Cuba, Chile, Colombia, Japan and France. "
,"D ->No bebo", "I like to party with people of Pakistan, Saudi Arabia, Morocco, Egipt, Turkey, Nicaragua, Ghana. "
]


Pregunta6=[
"6. ¿Si fueses un superhéroe, ¿Cuál sería??"
,"A ->Superman", "I am journalist. I have a lot of strength. I could fly. I wish I were plane pilot. I like helicopters. I do not like fame. "
,"B ->Spiderman" ,"I hace inhuman reactions, Agility, I like spiders. I like mountaineering. I love sports. I am orphan. I hate planes. I like radioactivity.  I study on college. I am shy. "
,"C ->Batman" , "I wish i were police or politician. I have no Powers. I have too much money. I am millonaire. I am funny. I need Friends. "
,"D ->Superheroe, ¿eso se come?", "I love sports. I hate videogames and movies. I am an uncommon person. I am comedian. "
]


Pregunta7=[
"7. ¿Qué persona eres en tu grupo de amigos?"
,"A ->El que primero se emborracha", "I am an impulsive person, very sensitive and manipulative. I am shy. I like alcohol. Alcoholic. I am musician. "
,"B ->El que se quiere ir pronto a casa" ,"I am a cold person. I am writer. I am asian. I love spend time with my family. I like philosophy. "
,"C ->El último en salir de la discoteca" , "I like to talk with people. I am outgoing. I like to party. I do sports. I hate silence. I am DJ. I am musician. I love going to festivals like tomorrowland. "
,"D ->El que sale de fiesta para ligar ", "I have many Friends. I am outgoing. I am seductive. I divorced. I never get married. I am funny. I am football player. I am psychologist. "
]


Pregunta8=[
"8. Que actividad prefieres en una fiesta?"
,"A ->Lo mío es bailar sin parar", "I love to dance more than anything. I am a dancer. I am energic. "
,"B ->Beber alcohol hasta que salga el sol como un vikingo" ,"My thing is to drink alcohol like a viking until the party is over. I am always drunk. I am alcoholic. I like to drink alcohol. "
,"C ->Prefiero perderme una buena conversacion a escuchar una mala cancion" , "I would rather miss a good conversation than listen to a bad song,  I will take care of the music. I am a musician. I love to sing. I love the art. "
,"D ->Charlar y conocer gente escuchando sus historias e intereses", "When I am at a party I love to listen to other people stories and interests and share mine’s. I am a big talker. I am a story teller. I am a good listener. "
]

Pregunta0=[
"0. Si pudieses llevar cualquier animal a una fiesta, cual llevarias?"
,"A ->Un águila real, el símbolo de EEUU", "Easy, a golden eagle, I could do anything with the symbol of the United States on my side! I like USA. I love americans. I like guns. "
,"B ->Un gran elefante" ,"The elephants allowed Anibal to confront the Roman Empire on their own territory, just imagine what could happen at a party. I like big animals. I like Africa. "
,"C ->Un lindo oso panda" , "The panda bear is the most adored animal in China, so as in the rest of Asia. I like cute animals. I like anime. I like sake. I like samurais. "
,"D ->Un canguro boxeador", "I like boxing. I like Australia. I like crocodiles. I am a fighter. I like Oceania. I like danger."
]

for i in range(9):
    actual = eval("Pregunta" + str(i))
    opcion = input(actual[0] + "\n" + actual[1] + "\n" + actual[3]  + "\n" + actual[5] + "\n" + actual[7] + "\n")
    correcto= 0;
    while(correcto == 0):
        if opcion == 'A' or opcion=='a':
            textofinal = textofinal  + actual[2] + "\n"
            correcto=1
        elif opcion == 'B' or opcion=='b':
            textofinal = textofinal  + actual[4] + "\n"
            correcto=1
        elif opcion == 'C' or opcion=='c':
            textofinal = textofinal  +actual[6] + "\n"
            correcto=1
        elif opcion == 'D' or opcion=='d':
            textofinal = textofinal  +actual[8] + "\n"
            correcto=1
        else:
            opcion= input("\n Las únicas opciones válidas son a, b, c y d. Prueba otra vez \n")
textFile= open("descripcionPersona.txt", "w")
n= textFile.write(textofinal)
textFile.close()
print("Tus resultados se han almacenado en descripcionPersona.txt")

"""# Recomendador

A continuación se añade una tupla al dataframe original mediante la creación y el adjunto de un dataset que incluye las respuestas en forma de cadena.
"""

import pandas as pd

searchTitle = open("descripcionPersona.txt", "r")
var = searchTitle.read().replace('\n', " ")
searchTitle.close()
print(var)

testData = pd.DataFrame({"name":["Test"], "Description":[var]})
df1 = pd.read_csv('final_dataframe.csv', engine='python',error_bad_lines=False)
notas = []
x = len(df1)
i = 0

while i < x:
  notas.append(100)
  i=i+1

df1['notas'] = notas

originalData = testData.append(df1.head(10000), ignore_index=True)

"""# Tokenization, stemmization y eliminación de stopwords

Antes de realizar la tokenizacion de la cadena con las respuestas del usuario, hay que eliminar las palabras que no aportan información relevante, las stopwords; y reducir otras a una base para facilitar el cálculo de vectores, es decir, aplicar el proceso de stemmization. Añadiremos el resultado de estas operaciones al datatext como una nueva columna: processed_text, aplicado tanto a las respuestas del usuario como a las descripciones de los personajes.
"""

from nltk.tokenize import word_tokenize 
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

import nltk
nltk.download('punkt')
nltk.download('stopwords')

ps = PorterStemmer()

preprocessedText = []
originalData['Description'] = originalData['Description'].apply(str)


for row in originalData.itertuples():
      text = word_tokenize(row[2]) ## indice de la columna que contiene el texto
      ## Remove stop words
      stops = set(stopwords.words("english"))
      text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]
      text = " ".join(text)
      preprocessedText.append(text)

preprocessedData = originalData
preprocessedData['processed_text'] = preprocessedText

preprocessedData

"""# Representación del texto como un vector

El siguiente paso es transformar las descripciones de los personajes en vectores de frecuencias (Bag of words), aplicando además la ponderación TF-IDF para los valores de dichas frecuencias(cuanto más se repite una palabra en el dataset, menos información aporta).
El paquete sklearn ofrece una clase llamada TfidfVectorizer que crea automáticamente la matriz compuesta por todos los vectores de frecuencias ponderados a partir de un array de textos (preprocessedData['processed_text'])
"""

from sklearn.feature_extraction.text import TfidfVectorizer

bagOfWordsModel = TfidfVectorizer()
bagOfWordsModel.fit(preprocessedData['processed_text'])
textsBoW= bagOfWordsModel.transform(preprocessedData['processed_text'])
print("Finished")

textsBoW.shape
print(textsBoW)

bagOfWordsModel.get_feature_names()

"""# Cálculo de distancia entre vectores de frecuencias (distancia coseno)

El objetivo final es el de calcular la distancia que existe entre vector de frecuencias del personaje y el de las respuestas del usuario.

Gracias a que ahora los textos están representados mediante vectores de frecuencias (textsBoW), se pueden emplear para ello medidas de distancias standard entre vectores. En este caso se usará la distancia coseno.
"""

from sklearn.metrics import pairwise_distances

distance_matrix= pairwise_distances(textsBoW,textsBoW ,metric='cosine')
print(distance_matrix.shape)
print(type(distance_matrix))

"""# Cálculo de distancia entre vectores de frecuencias (distancia manhattan)

En esta sección comprobamos la diferencia entre el uso de la distancia coseno usada anteriormente y la distancia manhattan en este caso. El tiempo de ejecución aumenta considerablemente respecto al caso anterior, esto se debe a que para la métrica coseno se usa la implementación scikit-learn que es más rápida y tiene soporte para matrices dispersas, la métrica manhattan, en cambio, no se implementa de esa forma.
"""

from sklearn.metrics import pairwise_distances

distance_matrix= pairwise_distances(textsBoW,textsBoW ,metric='manhattan')
print(distance_matrix.shape)
print(type(distance_matrix))

"""# Búsqueda de los personajes más aproximados según la respuesta del usuario

En este último paso listamos los 5 personajes del dataSet cuyos vectores de frecuencias tienen una menor distancia al vector de las respuestas del usuario; es decir, los 5 personajes con una descripción aproximada a la elegida en el test.
"""

distance_scores = list(enumerate(distance_matrix[0]))
i=1
ponderado =[]
ponderado.append((0, 0.0))

while i < len(distance_scores):
  valorPonderado= (100*distance_scores[i][1])*0.8 + originalData.iloc[i]['notas']*0.2
  ponderado.append((i,valorPonderado))
  i = i+1
  
distance_scores=ponderado
ponderado

distance_scores

ordered_scores = sorted(distance_scores, key=lambda x: x[1])
top_scores = ordered_scores[1:6]

top_indexes = [i[0] for i in top_scores]
res = preprocessedData['name'].iloc[top_indexes]
res

"""# **SPRINT 4**

En este sprint entrenaremos nuestro algoritmo de recomendación usando 2 nuevos dataframes compuestos por una serie de "tweets" etiquetados previamente segun sean más relevantes (-1 menos relevante y 1 más relevante). Gracias a esto podremos realizar el **análisis de sentimientos** teniendo en cuenta las valoraciones del usuario para usarlas en un sistema de puntuaciones.

# Preprocesamiento de los datos de entrenamiento

En este paso, como ya se hizo en el anterior sprint, leemos el dataframe (en este caso con el primer dataset de "tweets"),y procesamos el propio "tweet" alojado en la columna 'text' tokenizándolo, eliminando las stopwords y stemmizándolo.
"""

trainingData = pd.read_csv('semeval-2017-train.csv', delimiter='	')
trainingData = trainingData.head(1000) #Eliminar la funcion head() si se quiere usar todo el dataset. Para las pruebas usamos únicamente los 1000 primeros tweets
trainingData

"""Aquí se muestran todos los "tweets" en los 1000 primeros "tweets" por cada vallor de la etiqueta 'label'."""

trainingData['label'].value_counts()

from nltk.tokenize import word_tokenize 
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

import nltk
nltk.download('punkt')
nltk.download('stopwords')

ps = PorterStemmer()

preprocessedText = []

for row in trainingData.itertuples():
    
    
    text = word_tokenize(row[2]) ## indice de la columna que contiene el texto
    ## Remove stop words
    stops = set(stopwords.words("english"))
    text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]
    text = " ".join(text)
    
    preprocessedText.append(text)

preprocessedData = trainingData
preprocessedData['processed_text'] = preprocessedText

preprocessedData

"""# Creación de la bolsa de palabras

Ahora creamos creamos los vectores de frecuencias que contienen el texto procesado correspondiente a los "tweets" originales.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

bagOfWordsModel = TfidfVectorizer()
bagOfWordsModel.fit(preprocessedData['processed_text'])
textsBoW= bagOfWordsModel.transform(preprocessedData['processed_text'])
print("Finished")

textsBoW.shape

"""# Entrenamiento de un algoritmo de clasificación (SVM)

Para entrenar el algoritmo debemos separar la parte correspondiente a la bagOfWords (los datos de prueba) y los datos de entrenamiento. A continuación se entrena el algoritmo utilizando un fit(datos de prueba, datos de entrenamiento).
"""

from sklearn import svm
svc = svm.SVC(kernel='linear') #Modelo de clasificación

X_train = textsBoW #Documentos
Y_train = trainingData['label'] #Etiquetas de los documentos 
svc.fit(X_train, Y_train) #Entrenamiento

"""# Carga y preprocesado de documentos de test

Leemos y preprocesamos los datos del segundo dataframe de "tweets", escogiendo los 10000 primeros para conseguir más datos y con ello más precisión en las predicciones.
"""

testData = pd.read_csv('semeval-2017-test.csv', delimiter='	')
testData = testData.head(10000)
testData

ps = PorterStemmer()

preprocessedText = []

for row in testData.itertuples():
    
    
    text = word_tokenize(row[2]) ## indice de la columna que contiene el texto
    ## Remove stop words
    stops = set(stopwords.words("english"))
    text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]
    text = " ".join(text)
    
    preprocessedText.append(text)

preprocessedDataTest = testData
preprocessedDataTest['processed_text'] = preprocessedText

preprocessedDataTest

"""En este paso se muestra el número de "tweets" por cada etiqueta sobre los 10000 del segundo dataframe y se crean nuevos vectores de frecuencias utilizando la misma representación de bolsa de palabras que se usó para entrenar el algoritmo.


"""

testData['label'].value_counts()

textsBoWTest= bagOfWordsModel.transform(preprocessedDataTest['processed_text'])
print("Finished")
textsBoWTest.shape

"""# Clasificación de los documentos de test"""

X_test = textsBoWTest #Documentos

predictions = svc.predict(X_test) #Se almacena en el array predictions las predicciones del clasificador
X_test

"""# Evaluación de la predicción

En el siguiente fragmento se muestran distintos valores como la precisión de las predicciones para cada una de las etiquetas.
"""

from sklearn.metrics import classification_report

Y_test = testData['label'] #Etiquetas reales de los documentos

print (classification_report(Y_test, predictions))

"""# Entrenamiento y Evaluación de otro algoritmo de clasificación: k-NN

En esta sección usamos el algoritmo de clasificación k-NN, que clasifica los elementos en base a la distancia de los k-vecinos más próximos. Le aportamosal algoritmo diferentes valores de k para comparar resultados.

## Clasificación con k = 2
"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=2)


neigh.fit(X_train, Y_train) 
predictions = neigh.predict(X_test) 

print (classification_report(Y_test, predictions))

"""## Clasificación con k=1"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=1)


neigh.fit(X_train, Y_train) 
predictions = neigh.predict(X_test) 

print (classification_report(Y_test, predictions))

"""## Clasificación con k=5"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=5)


neigh.fit(X_train, Y_train) 
predictions = neigh.predict(X_test) 

print (classification_report(Y_test, predictions))

"""A continuación introducimos una opinión (claramente negativa) de ejemplo, y tras aplicar el procedimiento habitual (preprocesado y transformación en vector de frecuencias) y realizar una predicción obtenemos el valor esperado.

## Predicciones y valoraciones
"""

opinion = "I hate it"
opinion=[opinion]
dfOpinion = pd.DataFrame()
dfOpinion['opinion'] = opinion
dfOpinion
ps = PorterStemmer()

preprocessedTextOpinion = []

for row in dfOpinion.itertuples():
    
    
    text = word_tokenize(row[1]) ## indice de la columna que contiene el texto
    ## Remove stop words
    stops = set(stopwords.words("english"))
    text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]
    text = " ".join(text)
    
    preprocessedTextOpinion.append(text)

preprocessedData = dfOpinion
preprocessedData['processed_text'] = preprocessedTextOpinion


textsBoWOpinion= bagOfWordsModel.transform(preprocessedData['processed_text'])
print("Finished")
textsBoWOpinion.shape
X_test = textsBoWOpinion #Documentos
predictions = svc.predict(X_test) #Se almacena en el array predictions las predicciones del clasificador
predictions[0]

"""El siguiente paso es realizar las predicciones sobre las opiniones del usuario; para ello se le pide que las introduzca para la noche y para cada uno de los personajes que se le han recomendado en forma de caracter único (B)ien, (M)al o (I)do not know y le pedimos también una opinión general en forma de texto, que se procesará posteriormente."""

val= []
i = 0

while i <5:

  inputCorrecto = False
  while inputCorrecto == False:
    print("¿Cuál es tu opinión acerca de " + res.iloc[i] + "? Bien(B)/Mal(M)/Idk(I)")
    aux = input()
    aux.lower()

    if aux == 'b' or aux== 'm' or aux == 'i':
      val.append(aux)
      i = i+1
      inputCorrecto = True

input2 = False 
while input2 == False:
  print("¿Cuál es tu opinión acerca de la noche? Bien(B)/Mal(M)/Idk(I)")
  aux = input()
  aux.lower()

  if aux == 'b' or aux== 'm' or aux == 'i':
    val.append(aux)
    i = i+1
    input2 = True
  opinion=input("¿Podrías decirnos que te parece la noche en unas frases breves? En inglés por favor")
  opinion.lower()
  dfOpinion = pd.DataFrame()
  dfOpinion['opinion'] = opinion
  ps = PorterStemmer()
  preprocessedTextOpinion = []

  for row in dfOpinion.itertuples(): 
    text = word_tokenize(row[1]) ## indice de la columna que contiene el texto
    ## Remove stop wordsa
    stops = set(stopwords.words("english"))
    text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]
    text = " ".join(text)
  preprocessedTextOpinion.append(text)
  preprocessedData = dfOpinion
  preprocessedData['processed_text'] = preprocessedTextOpinion
  textsBoWOpinion= bagOfWordsModel.transform(preprocessedData['processed_text'])
  print("Finished")
  textsBoWOpinion.shape
  X_test = textsBoWOpinion #Documentos
  predictions = svc.predict(X_test) #Se almacena en el array predictions las predicciones del clasificador
val

"""Por último le asigna un valor a cada opinión (B: 1, M:-1, I: 0) y se añade al ya existente en la columna notas para cada uno de los personajes recomendados; en el caso de la opinión general de la noche se añade ese valor a todos ellos. De esta forma conseguimos hacer más o menos populares a los personajes en función de estas valoraciones."""

j = 0

while j <5:  
  col = originalData.loc[:, 'name'] == res.iloc[j]
  nota = originalData[col]['notas']

  if val[j] == 'm':
    nota = nota -1
  elif val[j] == 'b':
    nota = nota +1.5
  elif val[j] =='i':
    nota = nota -0.5
  nota = nota + predictions[0]
  print(predictions[0])
  originalData.at[col,'notas'] = nota
  j = j+1
  print(originalData[col]['notas'])